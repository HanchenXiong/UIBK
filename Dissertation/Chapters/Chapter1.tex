% Chapter 1

\chapter{Introduction} % Main chapter title
\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 
\lhead{Chapter 1. \emph{Introduction}} % This is for the header on each page - perhaps a shortened title

\rule{\textwidth}{0.4pt} \\[0.5cm]
\textit{``Before the begining of great brilliance, there must be chaos."}

\begin{flushright}
I Ching
\end{flushright}
\rule{\textwidth}{0.4pt} 
%----------------------------------------------------------------------------------------
\\
This is an introductory chapter which provides an overview of the thesis. As the title suggests, this thesis focuses on working with structural data, including 
inference, learning and optimization. In section \ref{sec:motivation}, some motivation and background of studying structure domains are explained. In section \ref{sec:overview}, 
an outline of the thesis is presented. Basically, the thesis is composed of three parts and each part is briefly introduced. In section \ref{sec:contribution}, as the author's 
contribution to relevant scientific fields, his research papers during Phd study are listed and summarized.       


\section{Work on Structure Domains}
\label{sec:motivation}
This section mainly explains the concept of "structure" and some structure domains which will be studied later.                  

Generally, structures exist almost everywhere in the universe. For instance, it is usually said that a galaxy, an architecture or a society is a structure.          
Looking closer at these three examples and abstracting their similarities, a rough definition of the "structure" can somehow be derived:     

\textit{A structure is is a set of elements, which exist with certain status based on the interactions among them. }  

The "interactions" can be also replaced with other terms at different scenarios, \emph{e.g.} dependencies, compatibilities, constraints. 
Tow critical points in understanding the definition are: (\emph{i}). there should be more than one element; (\emph{ii}). each element's status can be affected by the statuses of other ones.  
Based on this specification, 
more "structure" 
examples can also be easily observed. A sentence is a structure by considering words as its elements, an image is a structure as a set of dependent pixels, a human body is a structure composed of 
arms, legs and other parts, just name a few. Indeed, structure domains is very common in many areas.  



In old times, people are already aware of integrating data collected from different sources to find internal patterns among them.     
However, this procedure was usually slow and unstable.  
With the development of modern computers and digitization, increasingly more structural data are available.   



In these thesis,  structures are dealt with in three forms: (\emph{i}). graphs, (\emph{ii}). kernels, (\emph{iii}). manifolds.    

\section{Overview}
\label{sec:overview}
The thesis can be divided into three parts: inference, learning and optimization. The first part contains Chapter \ref{Chapter2}, where (probabilistic) graphical models 
and corresponding inference are presented. Chapter \ref{Chapter3} and Chapter \ref{Chapter4} belong to the second part, where two principles
for handling structures, \emph{kernels} and \emph{graphs}, are studied respectively. The third part contains Chapter \ref{Chapter5}, which studies the optimization on 
\emph{graphs} and \emph{matrix manifold}.                    

\subsection{Part I: Inference with Graphical Models}

\subsection{Part II: Structural Output Learning}

\subsection{Part III: Optimization on Structures}

\section{Author's Contribution}
\label{sec:contribution}
During the author's  PhD study, ten relevant research papers were finished, out of which nine are published (or accepted for publication) in 
journals or conference proceedings. They are listed as follows:     
\begin{shaded}
{\Huge I.} \textbf{Hanchen Xiong}, Sandor Szedmak, Justus Piater. {\it Implicit Learning of Simpler Output Kernels for Multi-Lable Prediction}, NIPS workshop on Representation and Learning Methods for Complex Outputs (NIPS-RLCO2014).  
\vspace{-.2cm}

{\Huge II.} \textbf{Hanchen Xiong}, Sandor Szedmak, Justus Piater. {\it Towards Maximum Likelihood: Learning Undirected Graphical Models using Persistent Sequential Monte Carlo}, The 6th Asian Conference on Machine Learning (ACML2014), 
\textbf{Best Paper Award}.    
\vspace{-.2cm}

{\Huge III.} \textbf{Hanchen Xiong}, Sandor Szedmak, Justus Piater. {\it Scalable, Accurate Image Annotation with Joint SVMs and Output Kernels}, Neurocomputing Journal (Accepted).  
\vspace{-.2cm}

{\Huge IV.} \textbf{Hanchen Xiong}, Sandor Szedmak, Antonio Rodr{\'i}guez S{\'a}nchez, Justus Piater. {\it Towards Sparsity and Selectivity: Bayesian Learning of Restricted Boltzmann Machine for Early Visual Features},In Proceedings of the 24th International Conference on Artificial Neural Networks (ICANN14), 2013, Springer. 
\vspace{-.2cm}

{\Huge V.} \textbf{Hanchen Xiong}, Sandor Szedmak, Justus Piater. {\it Joint SVM for Accurate and Fast Image Tagging},In Proceedings of the 22nd European Symposium on Artificial Neural Network (ESANN14). 
\vspace{-.2cm}


{\Huge VI.} \textbf{Hanchen Xiong}, Sandor Szedmak, Justus Piater. {\it 3D Object Class Geometry Modeling with Spatial Latent Dirichlet Markov Random Fields}, In Proceedings of the 35th German Conference on Pattern Recognition (GCPR13), pp 51-60, 2013,  Springer.  
\vspace{-.2cm}

{\Huge VII.} \textbf{Hanchen Xiong}, Sandor Szedmak, Justus Piater {\it Homogeneity Analysis for Object-Action Relations Reasoning in Kitchen Scenarios}, 
In Proceedings of 2nd Workshop on Machine Learning for Intelligent Systems (MLIS13), pp 37-44,  2013, ACM. 
\vspace{-.2cm}

  {\Huge VIII.} \textbf{Hanchen Xiong}, Sandor Szedmak, Justus Piater {\it A Study of Point Cloud Registration with Probability Product Kernel Functions}, 
In Proceedings of 2013 International Conference on 3D Vision (3DV13), pp 207-214,  2013, IEEE.
\vspace{-.2cm}

 {\Huge IX.} \textbf{Hanchen Xiong}, Sandor Szedmak, Justus Piater {\it Efficient, General Point Cloud Registration With Kernel Feature Maps}, 
In Proceedings of 10th International Conference on Computer and Robot Vision (CRV13), pp 83-90, 2013, IEEE.

\vspace{-.2cm}
\end{shaded}
To minimize content redundancy, only a subset of them are inserted at appropriate (sub)chapters of the thesis. In addition, to keep them self-contained and self-consistent,     
their original content and formats are preserved.   
Which should be distinguished from regular paragraphs. The preprints of these publications can be also found at: \url{http://iis.uibk.ac.at/publications}, complying with their corresponding copyrights. 

Besides, one unpublished work is also be presented in the thesis.   
\begin{shaded}
 {\Huge X.} \textbf{Hanchen Xiong}, Sandor Szedmak, Justus Piater {\it Multi-Label Learning with Kernel Generalized Homogeneity Analysis}, 
Unpublished, 2015.
\end{shaded}
Throughout the thesis, all these papers will be referred to by their corresponding roman numerals specified above.    
